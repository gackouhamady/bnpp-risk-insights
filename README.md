# ğŸš€ Project Objective

Prototype a **DataHub Risk & Customer Insights** for BNP Paribas CoE Data Science, covering:

* **Data ingestion & preparation** (accounts, transactions, KYC) in Python & SQL
* **Building a Risk & Customer Analytics datamart** (star schema)
* **Interactive reporting & dashboards** with Power BI or Tableau
* **Predictive modeling**: default scoring (Logistic Regression) and churn (XGBoost)
* **Transaction anomaly detection** (Isolation Forest)
* **Lightweight MLOps pipeline** (Airflow or batch scripts + MLflow)
* **Documentation & best practices** (versioning, testing, GDPR/anonymization)

---

## ğŸ“… Detailed 5-day working plan (+ 2 days buffer)



|                                                  Day                                                 | Key tasks                                                      |
| :--------------------------------------------------------------------------------------------------: | :------------------------------------------------------------- |
|                                               **Day 1**                                              | - **Init repo & env**: `git init` + `.gitignore` + Python venv |
| â€¢ `requirements.txt` (pandas, numpy, scikit-learn, sqlalchemy, mlflow, fastapi, uvicorn, streamlitâ€¦) |                                                                |

* **Data sampling**: simulated CSVs of client accounts, transactions, and KYC file (1,000 rows each) |
  \| **Day 2** | - **ETL & datamart** (`src/etl.py`):
  â€¢ Extraction CSV â†’ Pandas â†’ cleaning (formats, duplicates)
  â€¢ Loading into local SQLite or Postgres
  â€¢ Creation of fact tables (`transactions`, `events`) and dimensions (`clients`, `accounts`, `time`) |
  \| **Day 3** | - **Reporting & BI** (`src/reporting.py`):
  â€¢ Prototype Power BI/Tableau dashboard: key KPIs (transaction volume, active portfolio)
  â€¢ Automate CSV exports via Python/VBA for daily refresh |
  \| **Day 4** | - **Predictive modeling**:
  â€¢ `src/model_default.py` â€“ Logistic Regression for payment default (features: average amount, frequencyâ€¦)
  â€¢ `src/model_churn.py` â€“ XGBoost to predict churn (features: tenure, average balanceâ€¦)
  â€¢ Evaluation: AUC, confusion matrix, classification report |
  \| **Day 5** | - **Anomaly detection** (`src/anomaly.py`):
  â€¢ Isolation Forest on transaction amounts & frequencies
  â€¢ Visualization of outliers in Streamlit
* **Pipeline & MLOps** (`src/pipeline.py`):
  â€¢ Orchestration ETL â†’ scoring â†’ anomaly â†’ JSON report generation
  â€¢ MLflow experiment tracking (params & metrics) |
  \| **Day 6** | - **Interface prototype** (`src/app.py`):
  â€¢ Local Streamlit to view dashboards, run scoring & anomaly
  â€¢ FastAPI exposing endpoints `/score_default`, `/detect_anomaly` |
  \| **Day 7** | - **Documentation & tests**:
  â€¢ `README.md` (installation, usage, Dataiku/GCP migration)
  â€¢ ER diagram in `docs/`
  â€¢ Pytest unit tests for each module
* **Packaging & delivery**:
  â€¢ Final `requirements.txt` (`pip freeze`)
  â€¢ Dockerfile (base `python:3.10`) for Streamlit + FastAPI app
  â€¢ Commit & push to GitHub + Cloud Run/Vertex AI deployment plan |

---

## âš™ï¸ Final Git structure

```bash
bnpp-risk-insights/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # simulated accounts, transactions, KYC CSVs
â”‚   â””â”€â”€ processed/                 # cleansed tables
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ datamart_schema.png        # star schema diagram
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl.py                     # extraction, cleaning, loading
â”‚   â”œâ”€â”€ reporting.py               # CSV exports & BI templates
â”‚   â”œâ”€â”€ model_default.py           # default scoring (Logistic Regression)
â”‚   â”œâ”€â”€ model_churn.py             # churn (XGBoost)
â”‚   â”œâ”€â”€ anomaly.py                 # Isolation Forest
â”‚   â”œâ”€â”€ pipeline.py                # orchestration + MLflow
â”‚   â””â”€â”€ app.py                     # Streamlit + FastAPI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_etl.py
â”‚   â”œâ”€â”€ test_reporting.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```



## Architecture summary  

``` text

1. Data Sources
   â”œâ”€ Client accounts (CSV)
   â”œâ”€ Transactions (CSV)
   â””â”€ KYC (CSV)

2. Ingestion & Preparation (ETL)
   â”œâ”€ Extraction (pandas)
   â”œâ”€ Cleaning (formats, duplicates, GDPR)
   â””â”€ Loading (SQLAlchemy â†’ SQLite/Postgres)

3. Datamart (star schema)
   â”œâ”€ Fact tables
   â”‚   â”œâ”€ transactions
   â”‚   â””â”€ events
   â””â”€ Dimension tables
       â”œâ”€ clients
       â”œâ”€ accounts
       â””â”€ time

4. Storage & Traceability
   â”œâ”€ Optimized SQL database
   â””â”€ MLflow (tracking runs: parameters, metrics, artifacts)

5. Analytical Modules
   â”œâ”€ Default scoring (logistic regression)
   â”œâ”€ Churn prediction (XGBoost)
   â””â”€ Anomaly detection (Isolation Forest)

6. MLOps Orchestration
   â”œâ”€ Airflow or batch scripts
   â””â”€ Pipeline ETL â†’ scoring â†’ anomaly â†’ JSON reports

7. Interfaces
   â”œâ”€ Streamlit (dashboards & manual execution)
   â””â”€ FastAPI (endpoints `/score_default` & `/detect_anomaly`)

8. Containerization & Deployment
   â”œâ”€ Docker (Python 3.10)
   â””â”€ Cloud Run / Vertex AI (auto-scaling, monitoring)

9. Quality & Governance
   â”œâ”€ Unit tests (Pytest)
   â”œâ”€ Documentation (README, ER diagram)
   â””â”€ Git conventions and code reviews

```