# ğŸš€ DataHub Risk & Customer Insights

**Prototype** developed for BNP Paribas CoE Data Science:  
â€“ Data ingestion & preparation  
â€“ Analytics datamart (star schema)  
â€“ Interactive reporting & dashboards  
â€“ Predictive modeling (default scoring & churn prediction)  
â€“ Transaction anomaly detection  
â€“ Lightweight MLOps pipeline (scripts + MLflow)  
â€“ REST API (FastAPI) & Streamlit UI  
â€“ Docker container   

---

## ğŸ“Š Model Performance Summary

| Model                   | AUC    | Precision (0 / 1) | Recall (0 / 1) | F1-score (0 / 1) | Accuracy |
|-------------------------|--------|-------------------|----------------|------------------|----------|
| **Default Scoring**     | 0.9981 | 0.97 / 0.97       | 0.99 / 0.89    | 0.98 / 0.93      | 0.97     |
| **Churn Prediction**    | 0.9167 | 0.96 / 1.00       | 1.00 / 0.83    | 0.98 / 0.91      | 0.97     |

> **Default Scoring** (Logistic Regression)  
> - AUC 0.9981 â†’ nearly perfect separation  
> - Accuracy 97% on 192 samples  

> **Churn Prediction** (XGBoost)  
> - AUC 0.9167 â†’ excellent churn vs. retention discrimination  
> - Accuracy 97% on 30 samples  

---

## ğŸ“… Detailed Day-by-Day Plan

| **Day** | **Deliverables**                                                                                                                                     |
|---------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| **1**   | â€“ Repo & venv initialization, `.gitignore` <br> â€“ `requirements.txt` <br> â€“ Generate 1 000 synthetic rows (accounts, transactions, KYC)               |
| **2**   | â€“ ETL & datamart (`src/etl.py`): extract raw CSVs, clean, star schema creation, load into SQLite/Postgres                                             |
| **3**   | â€“ Reporting & BI (`src/reporting.py`): timestamped CSV exports & Power BI/Tableau prototype <br> â€“ Daily refresh script                              |
| **4**   | â€“ Default scoring (`src/model_default.py`) <br> â€“ Churn prediction (`src/model_churn.py`) <br> â€“ AUC, confusion matrix, classification report         |
| **5**   | â€“ Anomaly detection (`src/anomaly.py`): Isolation Forest + Streamlit UI <br> â€“ Partial orchestration                                                   |
| **6**   | â€“ Full pipeline (`src/pipeline.py`): ETL â†’ default & churn scoring â†’ anomalies â†’ MLflow â†’ JSON report <br> â€“ FastAPI (`src/app.py`) endpoints           |
| **7**   | â€“ Write `README.md` <br> â€“ ER diagram (`docs/datamart_schema.png`) <br> â€“ Unit tests (Pytest) <br> â€“ Dockerfile (Python 3.10) <br> â€“ Cloud deployment plan |

---

## âš™ï¸ Repository Structure

```bash
bnpp-risk-insights/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # synthetic CSVs: accounts, transactions, kyc
â”‚   â”œâ”€â”€ processed/           # SQLite datamart (risk_insights.db)
â”‚   â””â”€â”€ exports/             # timestamped CSVs for BI
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ datamart_schema.png  # star schema ER diagram
â”‚   â””â”€â”€ anomaly_ui.png       # Streamlit anomaly detection screenshot
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logreg_default.pkl   # default scoring model
â”‚   â””â”€â”€ xgb_churn.json       # churn prediction model
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ report_YYYYMMDD_HHMMSS.json  # JSON reports per run
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_data.py     # synthetic data generator
â”‚   â”œâ”€â”€ etl.py               # extract, transform, load
â”‚   â”œâ”€â”€ reporting.py         # CSV exports & BI prep
â”‚   â”œâ”€â”€ model_default.py     # logistic regression scoring
â”‚   â”œâ”€â”€ model_churn.py       # XGBoost churn prediction
â”‚   â”œâ”€â”€ anomaly.py           # Isolation Forest + Streamlit UI
â”‚   â”œâ”€â”€ pipeline.py          # orchestration + MLflow + JSON report
â”‚   â””â”€â”€ app.py               # FastAPI application
â”œâ”€â”€ tests/                   # Pytest unit tests
â”œâ”€â”€ Dockerfile               # multi-stage Python 3.10 container
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # this file
```



# 1. Clone & prepare venv
git clone repo
python -m venv venv
# Windows
.\venv\Scripts\Activate.ps1
 

# 2. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 3. Generate sample data
python src/generate_data.py

# 4. Build the datamart
python src/etl.py

# 5. Export for BI
python src/reporting.py

# 6. Train models
python src/model_default.py
python src/model_churn.py

# 7. Run anomaly UI
streamlit run src/anomaly.py

<img src="docs/anomaly.png" alt="Streamlit Anomaly UI" width="700"/>

# 8. Orchestrate & generate report
python src/pipeline.py

# 9. Start the API
python -m uvicorn src.app:app --reload --host 0.0.0.0 --port 8000
# Then open http://localhost:8000/docs

# 10. Run unit tests
pytest --maxfail=1 -q

# 11. Docker 
docker build -t risk-insights:latest .
docker run -p 8000:8000 risk-insights:latest
 



# ğŸ”’ Governance & Ethics
Synthetic data onlyâ€”no real PII.

GDPR-compliant: anonymization, privacy-by-design.

Transparent, versioned code with unit tests.

Ready for industrialization in Dataiku / GCP.

A production-ready blueprint for risk & customer analytics in banking.
